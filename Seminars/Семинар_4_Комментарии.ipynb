{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8F8_NNT54Zf",
        "outputId": "fd20be43-ad53-4080-e974-69494f06aa71"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "asJ5h-5q8LA2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJONvAix5zzX"
      },
      "source": [
        "### Задание 1\n",
        "Вывод данных в консоль\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTvqNZo758jp"
      },
      "source": [
        "1. Создаем сессию Spark с именем \"CountElements\".\n",
        "2. Читаем поток данных с использованием источника \"rate\", который автоматически генерирует данные. Этот источник используется для тестирования и отладки, поскольку он позволяет легко создавать потоки данных без необходимости подключения к внешним системам.\n",
        "3. Записываем полученные данные в консоль с использованием режима вывода \"append\". В этом режиме в консоль будут выводиться только новые строки, полученные в каждом микро-пакете данных.\n",
        "4. Запускаем поток обработки данных и ожидает его завершения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5K_V_6Kf3-nT"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o43.start.\n: java.io.IOException: Cannot run program \"C:\\hadoop\\bin\\winutils.exe\": CreateProcess error=216, Эта версия \"%1\" не совместима с версией Windows, работающей на этом компьютере. Проверьте сведения о системе, а затем обратитесь к издателю программного обеспечения\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1170)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1089)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:995)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:959)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1282)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1377)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1359)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:414)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:577)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:566)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:597)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1356)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:107)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:376)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:423)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:638)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:705)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:365)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:141)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:144)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:371)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:79)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:160)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:158)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:52)\r\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:230)\r\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:281)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startQuery(DataStreamWriter.scala:314)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:291)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: CreateProcess error=216, Эта версия \"%1\" не совместима с версией Windows, работающей на этом компьютере. Проверьте сведения о системе, а затем обратитесь к издателю программного обеспечения\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:506)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1126)\r\n\t... 47 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m spark = SparkSession.builder.appName(\u001b[33m\"\u001b[39m\u001b[33mCountElements\u001b[39m\u001b[33m\"\u001b[39m).getOrCreate()\n\u001b[32m      4\u001b[39m df = spark.readStream.format(\u001b[33m\"\u001b[39m\u001b[33mrate\u001b[39m\u001b[33m\"\u001b[39m).load()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m query = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriteStream\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconsole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m query.awaitTermination()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:1704\u001b[39m, in \u001b[36mDataStreamWriter.start\u001b[39m\u001b[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[39m\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28mself\u001b[39m.queryName(queryName)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28mself\u001b[39m._jwrite.start(path))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
            "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o43.start.\n: java.io.IOException: Cannot run program \"C:\\hadoop\\bin\\winutils.exe\": CreateProcess error=216, Эта версия \"%1\" не совместима с версией Windows, работающей на этом компьютере. Проверьте сведения о системе, а затем обратитесь к издателю программного обеспечения\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1170)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1089)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:995)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:959)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1282)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1377)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1359)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:414)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:577)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:566)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:597)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1356)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:107)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:376)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:423)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:638)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:705)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:365)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:141)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:144)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:371)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:79)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:160)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:158)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:52)\r\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:230)\r\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:281)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startQuery(DataStreamWriter.scala:314)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:291)\r\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.IOException: CreateProcess error=216, Эта версия \"%1\" не совместима с версией Windows, работающей на этом компьютере. Проверьте сведения о системе, а затем обратитесь к издателю программного обеспечения\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:506)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1126)\r\n\t... 47 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CountElements\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6yBx_7h7GeD"
      },
      "source": [
        "### Задание 2:\n",
        "Фильтрация чисел (четные)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q_5imWc4i7h"
      },
      "source": [
        "1. Создаем сессию Spark с именем приложения \"FilterEvenNumbers\". Это необходимо для инициализации Spark и подготовки к выполнению операций с данными.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и демонстрации обработки потоковых данных.\n",
        "\n",
        "3. Фильтруем полученные данные, оставляя только те строки, где значение поля \"value\" делится на 2 без остатка, то есть числа, являющиеся четными.\n",
        "\n",
        "4. Записываем отфильтрованные данные обратно в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоке, новые данные будут добавлены к уже существующим, не удаляя предыдущие записи.\n",
        "\n",
        "5. Запускаем потоковую запись и ожидает завершения работы запроса в течение 5 секунд. Это позволяет увидеть результаты обработки данных в консоли в течение этого времени.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrT_u-Te6O_u",
        "outputId": "8069bc9d-cded-47bc-a006-929f87846386"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"FilterEvenNumbers\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_even = df.filter(\"value % 2 == 0\")\n",
        "query = df_even.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgLHHmE7f9D"
      },
      "source": [
        "### Задание 3:\n",
        "Сгруппировать по значениям и посчитать количество\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCsmYvdT4vzL"
      },
      "source": [
        "1. Создаем сессию Spark с именем приложения \"GroupByValue\". Это необходимо для работы с Spark и его функциональностью, включая потоковую обработку данных.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с постоянной скоростью, что удобно для тестирования и демонстрации потоковой обработки.\n",
        "\n",
        "3. Группируем полученные данные по столбцу \"value\" и подсчитывает количество записей для каждого уникального значения в этом столбце. Это позволяет агрегировать данные по определенному критерию.\n",
        "\n",
        "4. Запускаем потоковую запись результатов агрегации в консоль с использованием режима вывода \"update\". Режим \"update\" означает, что в консоль будут выводиться только обновленные результаты агрегации, а не полный набор данных при каждом обновлении. Это делает вывод более читаемым и эффективным, особенно когда данные постоянно обновляются.\n",
        "\n",
        "5. Ожидаем завершения потоковой обработки в течение 5 секунд. Это означает, что поток будет работать в течение этого времени, а затем завершится.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNG16a4I7LFA",
        "outputId": "588fdabd-18a8-4d9f-e462-249a6da8e2a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"GroupByValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_grouped = df.groupBy(\"value\").count()\n",
        "query = df_grouped.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-mPJYiE7mKI"
      },
      "source": [
        "### Задание 4\n",
        "Вычислить сумму значений\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpp4yFv48Qt"
      },
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"SumValues\". Этот шаг необходим для инициализации Spark и подготовки среды для выполнения операций с данными.\n",
        "\n",
        "2. Читаем поток данных из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки. В данном случае, данные будут генерироваться автоматически без необходимости подключения к внешним источникам данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя сумму значений в столбце \"value\" и сохраняя результат в новом столбце \"total\". Это делается с помощью метода selectExpr, который позволяет выполнять SQL-подобные выражения для трансформации данных.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием формата \"console\". Это позволяет наблюдать за изменениями в реальном времени, так как данные будут выводиться в консоль.\n",
        "\n",
        "5. Запускаем запрос на потоковую обработку данных с использованием режима вывода \"update\". В этом режиме, при каждом обновлении данных, будет выводиться только текущее состояние агрегации, что позволяет видеть актуальную сумму значений.\n",
        "\n",
        "6. Ожидаем завершения обработки потока данных в течение 5 секунд с помощью метода awaitTermination. Это означает, что приложение будет работать в течение этого времени, обрабатывая поступающие данные и выводя результаты в консоль.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xiJZWI6f7lzl",
        "outputId": "b13d34ef-8c34-4a6e-a242-77611147035a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SparkSession' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f6543506e4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SumValues\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum(value) AS total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"SumValues\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_sum = df.selectExpr(\"sum(value) AS total\")\n",
        "query = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSk3qT1I7vCi"
      },
      "source": [
        "### Задание 5\n",
        "Найти максимальное значение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4YRbmkS5J4X"
      },
      "source": [
        "1. Создаем сессию Spark с именем приложения \"MaxValue\" с помощью SparkSession.builder.appName(\"MaxValue\").getOrCreate(). Это необходимо для работы с Spark и его функциональностью, включая структурированный потоковый анализ.\n",
        "\n",
        "2. Читаем поток данных с использованием источника \"rate\". Источник \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и разработки. В данном случае, данные читаются без дополнительных параметров, что означает использование значений по умолчанию для генерации данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя максимальное значение поля 'value' в каждом микро-батче данных. Это достигается с помощью метода agg(f.max('value')), где f - это ссылка на модуль pyspark.sql.functions, который предоставляет функции для работы с данными.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием writeStream.outputMode(\"update\").format(\"console\").start(). Здесь outputMode(\"update\") указывает, что при каждом обновлении данных в потоке, результаты агрегации будут выводиться в консоль. format(\"console\") указывает, что вывод должен быть направлен в консоль, а не в файл или базу данных.\n",
        "\n",
        "5. Ожидаем завершения потокового запроса с помощью query.awaitTermination(5), что означает, что поток будет работать в течение 5 секунд, после чего будет выполнено завершение работы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjPrxvmF5QKc"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"MaxValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_max = df.agg(f.max('value'))\n",
        "query = df_max.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkgaFpi472pT"
      },
      "source": [
        "### Задание 6\n",
        "Вычислить скользящее окно по значению\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2wCQyBC5aD6"
      },
      "source": [
        "1. Создаем сессию Spark с именем \"SlidingWindow\".\n",
        "2. Читаем потоковые данные из источника \"rate\", который генерирует данные с постоянной скоростью.\n",
        "3. Группируем эти данные по временным окнам, размером в 10 минут, и подсчитывает количество записей в каждом окне.\n",
        "4. Записываем результаты обработки в консоль в режиме обновления (outputMode(\"update\")), что означает, что в консоль будут выводиться только обновленные агрегированные результаты.\n",
        "5. Запускаем потоковую запрос и ожидает его завершения в течение 5 секунд.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgGCMPhD5Rnp"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"SlidingWindow\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_windowed = df.groupBy(window(\"timestamp\", \"10 minutes\")).count()\n",
        "query = df_windowed.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Tm_dbj79n8"
      },
      "source": [
        "### Задание 7\n",
        "Соединение потоков данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013d5A_d5luj"
      },
      "source": [
        "1. Создаем сессию Spark с именем приложения \"JoinStreams\".\n",
        "2. Читаем два потока данных, используя формат \"rate\", который генерирует данные с определенной скоростью. По умолчанию, каждый поток будет генерировать одну строку в секунду с полями timestamp и value.\n",
        "3. Соединяем два потока данных по полю value. Это означает, что для каждой пары строк из двух потоков, где значение поля value совпадает, будет создана новая строка в результате соединения.\n",
        "4. Записываем результат соединения в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоках, новые строки будут добавляться к выводу, не удаляя предыдущие строки.\n",
        "5. Запускаем запрос на потоковую обработку и ожидает его завершения в течение 5 секунд.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhpFy8Ut76yx"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"JoinStreams\").getOrCreate()\n",
        "df1 = spark.readStream.format(\"rate\").load()\n",
        "df2 = spark.readStream.format(\"rate\").load()\n",
        "df_joined = df1.join(df2, \"value\")\n",
        "query = df_joined.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzea5l1S8C9I"
      },
      "source": [
        "### Задание 8\n",
        "Запись данных в файл\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1-Pd1e5vE_"
      },
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"WriteToFile\". SparkSession является точкой входа для работы с Spark и предоставляет API для работы с DataFrame и DataSet.\n",
        "\n",
        "2. Читаем потоковые данные с использованием формата \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки потоковых приложений. В данном случае, данные будут генерироваться без дополнительных параметров, что означает использование значений по умолчанию.\n",
        "\n",
        "3. Записываем полученные потоковые данные в формате Parquet в указанный каталог \"output/\". Parquet - это эффективный формат хранения данных, который обеспечивает высокую производительность и эффективное сжатие.\n",
        "\n",
        "4. Запускаем потоковую запись данных и ожидает завершения работы запроса в течение 5 секунд. Метод awaitTermination блокирует выполнение программы до тех пор, пока потоковая запись не будет остановлена или не произойдет ошибка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqhFZzIB8DvS"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"WriteToFile\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream.format(\"parquet\").option(\"path\", \"output/\").start()\n",
        "query.awaitTermination(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
